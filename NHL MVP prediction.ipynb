{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NHL MVP Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is a first foray into machine learning. The goal is to predict player's vote share of the Hart trophy, which is awarded to the most valuable player in the National Hockey League as voted on by members of the Professional Hockey Writer's Association.  \n",
    "\n",
    "Hockey statistics come from Evolving Hockey and Hockey-Reference. At the moment only basic statistics provided in the Hart trophy candidates by year summary page from Hockey-Reference are used. A medium term goal is to scrape data including advanced statistics to combine with the existing data. As it will turn out, prediction performance based only on the basic statistics used to far is not very good. \n",
    "\n",
    "References/Acknowledgements:\n",
    "Most of what appears in this notebook was learned/borrowed from Kaggle. In particular, two specific notebooks I heavily referenced are https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python and https://www.kaggle.com/mmueller/stacking-starter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named xgboost",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6388e2d00937>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mExtraTreesRegressor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named xgboost"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "import csv \n",
    "import sklearn\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import warnings\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.ensemble import (RandomForestRegressor,  ExtraTreesRegressor)\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import (KFold, train_test_split, cross_val_score)\n",
    "from sklearn.metrics import (mean_absolute_error)\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "%matplotlib inline\n",
    "py.init_notebook_mode(connected=True)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    full_data= pd.read_csv(\"hockey-data-hart.csv\")    \n",
    "except:\n",
    "    # Scrape data\n",
    "    # specify the url\n",
    "    years = range(1936,2018 + 1)\n",
    "    addresses = [\"https://www.hockey-reference.com/awards/voting-{}.html#all-hart-stats\".format(yr) for yr in years]\n",
    "    names = ['Year',  \n",
    "             'player',\n",
    "             'age',\n",
    "             'team_id',\n",
    "             'pos',\n",
    "             'votes',\n",
    "             'pct_of_vote',\n",
    "             'first',\n",
    "             'second',\n",
    "             'third',\n",
    "             'fourth',\n",
    "             'fifth',\n",
    "             'goals',\n",
    "             'assists',\n",
    "             'points',\n",
    "             'plus_minus',\n",
    "             'wins_goalie',\n",
    "             'losses_goalie',\n",
    "             'ties_goalie',\n",
    "             'goals_against_avg',\n",
    "             'save_pct',\n",
    "             'ops',\n",
    "             'dps',\n",
    "             'gps',\n",
    "             'ps']\n",
    "\n",
    "    with open('hockey-data-hart.csv', 'w') as csvfile:    #Create the csv file\n",
    "        hockeywriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        hockeywriter.writerow(names)\n",
    "        for i, pg in enumerate(addresses):\n",
    "            time.sleep(0.1) #pause the code for a sec\n",
    "            r = requests.get(pg)\n",
    "            # parse the html using beautiful soup and store in variable `soup`\n",
    "            soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "            values = [years[i]]  #first value\n",
    "\n",
    "            #Go through the stats and add them to a list\n",
    "            for tag in soup.findAll(\"td\"):\n",
    "                if tag.string == 'None': #don't save 'None' string\n",
    "                    values.append('') \n",
    "                elif tag['data-stat']=='team_id': #use proper team name\n",
    "                    try: #though it may not exist\n",
    "                        values.append(tag.contents[0]['title'])\n",
    "                    except:\n",
    "                        values.append(tag.string)\n",
    "                else:\n",
    "                    values.append(tag.string)\n",
    "                if tag['data-stat']=='ps': #if last value in row start on next line\n",
    "                    hockeywriter.writerow(values)\n",
    "                    values = [years[i]]  #set first value again     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data= pd.read_csv(\"hockey-data-hart.csv\")    \n",
    "\n",
    "\n",
    "# clean it up\n",
    "full_data=full_data.dropna(axis=0,subset=['pct_of_vote'])\n",
    "full_data=full_data.drop(['first','second','third','fourth','fifth'],axis=1)\n",
    "full_data = full_data[full_data[\"pct_of_vote\"]>0]\n",
    "\n",
    "# let's also drop goalies who are missing save pct, and skaters who are missing plus_minus\n",
    "full_data = full_data.loc[~((full_data.pos==\"G\") & full_data.save_pct.isna())]\n",
    "full_data = full_data.loc[~((full_data.pos!=\"G\") & full_data.plus_minus.isna())]\n",
    "\n",
    "# remaining na's should be due to mismatched position with stats. set them to zero\n",
    "full_data = full_data.fillna(0)\n",
    "\n",
    "# for non goalies (goalies), let's replace all goalie (non goalie) stats by zero rather than NaN\n",
    "\n",
    "# Create a new feature p_simp: for players who have joint positions, we just keep the first one\n",
    "def pos_combine(name):\n",
    "    title_search = re.search('(^[A-Z]{1,2})', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "full_data[\"pos\"] = full_data[\"pos\"].apply(pos_combine)\n",
    "full_data = full_data[full_data[\"pct_of_vote\"]>1]\n",
    "\n",
    "skaters = full_data[full_data['pos']!='G']\n",
    "goalies = full_data[full_data['pos']=='G']\n",
    "   \n",
    "full_data.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_data.describe())\n",
    "full_data['pct_of_vote'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "sns.kdeplot(skaters['goals'])\n",
    "fig = plt.figure()\n",
    "sns.jointplot(x='goals',y='pct_of_vote',data = skaters, kind='kde', gridsize=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examining some skater data\n",
    "sns.pairplot(skaters[['plus_minus','pct_of_vote','age','goals', 'assists','ps']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examining some goalie data\n",
    "sns.pairplot(goalies[['pct_of_vote','age','wins_goalie','losses_goalie','ties_goalie','goals_against_avg','save_pct','ps']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(skaters,col=\"pos\")\n",
    "g.map(sns.scatterplot, 'age','points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = skaters\n",
    "features= ['pct_of_vote','goals','assists','plus_minus','age','ops','dps','pos']\n",
    "fd = fd[features]\n",
    "f = fd.corr()\n",
    "plt.figure(figsize=(len(features),len(features)))\n",
    "\n",
    "sns.heatmap(f,annot=True,cmap=\"RdBu_r\",vmin=-1,vmax=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = goalies\n",
    "features= ['pct_of_vote','age','wins_goalie','losses_goalie',\n",
    "           'ties_goalie','goals_against_avg','save_pct','ps']\n",
    "fd = fd[features]\n",
    "f = fd.corr()\n",
    "plt.figure(figsize=(len(features),len(features)));\n",
    "\n",
    "sns.heatmap(f,annot=True,cmap=\"RdBu_r\",vmin=-1,vmax=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can expect from the plots above that directly fitting a model to the available data might be challenging. Nonetheless we will try anyways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = full_data.copy()\n",
    "# prepare for applying model:\n",
    "# 1) preprocessing-scale\n",
    "# t = fd.drop([\"Year\"],axis=1).select_dtypes(exclude=[\"object\"]).apply(sklearn.preprocessing.scale)\n",
    "# fd[t.columns] = t\n",
    "# actually its incorrect to preprocess the entire data set. this results in data contamination\n",
    "\n",
    "\n",
    "# 2) one-hot encode positions\n",
    "fd=pd.get_dummies(fd,columns=['pos'])\n",
    "\n",
    "fd[fd[\"Year\"]==2018]\n",
    "# full_data.sample(10)\n",
    "# fd.pct_of_vote.plot.hist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_features = [ u'age', u'goals', u'assists',  u'plus_minus',\n",
    "       u'wins_goalie', u'losses_goalie', u'ties_goalie', u'goals_against_avg',\n",
    "       u'save_pct', u'ops', u'dps', u'gps', u'pos_C',\n",
    "       u'pos_D', u'pos_G', u'pos_LW', u'pos_RW']\n",
    "\n",
    "# Use all years 2015 and prior for training, and try to predict the previous three years.\n",
    "target_yr = 2015\n",
    "train=fd[fd[\"Year\"]<=target_yr]\n",
    "test=fd[fd[\"Year\"] > target_yr]\n",
    "\n",
    "\n",
    "# Prepare a holdout of the train set to use for blending. \n",
    "base, holdout = train_test_split(train,test_size=0.1)\n",
    "\n",
    "x_test = test[model_features]\n",
    "y_test = test.pct_of_vote\n",
    "\n",
    "x_base = base[model_features]\n",
    "y_base = base.pct_of_vote\n",
    "\n",
    "x_holdout = holdout[model_features]\n",
    "y_holdout = holdout.pct_of_vote\n",
    "\n",
    "\n",
    "# x_base and y_base will be used to fit the base models\n",
    "# then make predictions for y_holdout using x_holdout\n",
    "# those predictions are used to train the stacker\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # scaling preprocessing the data. I'm not sure if it's actually a good idea to do this\n",
    "# # Leave it out for now.\n",
    "\n",
    "# t = x_base.drop([\"pos_G\"],axis=1).select_dtypes(exclude=[\"object\"]).apply(sklearn.preprocessing.scale)\n",
    "# x_base[t.columns] = t\n",
    "\n",
    "# t = x_holdout.drop([\"pos_G\"],axis=1).select_dtypes(exclude=[\"object\"]).apply(sklearn.preprocessing.scale)\n",
    "# x_holdout[t.columns] = t\n",
    "\n",
    "# t = x_test.drop([\"pos_G\"],axis=1).select_dtypes(exclude=[\"object\"]).apply(sklearn.preprocessing.scale)\n",
    "# x_test[t.columns] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper class for sklearn models\n",
    "# It trains on logarithmically renormalized vote_pct, rather than raw.\n",
    "# This is because heuristically the vote share by player\n",
    "# in any given year is approximately exponentially decaying\n",
    "# so it made sense to me to try to make the data more regular by taking the log.\n",
    "class SklearnWrapper(object):\n",
    "    def __init__(self,clf,seed=0,params=None):\n",
    "        params['random_state'] = seed\n",
    "        self.clf = clf(**params)\n",
    "        \n",
    "        \n",
    "    # automatically does log conversion in train and predict\n",
    "    def train(self,x_train,y_train):\n",
    "        self.clf.fit(x_train,np.log(y_train))\n",
    "        \n",
    "    def predict(self,x):\n",
    "        return np.exp(self.clf.predict(x))\n",
    "    \n",
    "    def feature_importances(self):\n",
    "        print(clf.feature_importances_)\n",
    "        \n",
    "def get_base_predictions(clf):\n",
    "\n",
    "    clf.train(x_base,y_base)\n",
    "    return clf.predict(x_holdout)\n",
    "    \n",
    "# Wrapper for xgboost from sklearn courtesy of https://www.kaggle.com/mmueller/stacking-starter\n",
    "class XgbWrapper(object):\n",
    "    def __init__(self, seed=0, params=None):\n",
    "        self.param = params\n",
    "        self.param['seed'] = seed\n",
    "        self.nrounds = params.pop('nrounds', 250)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        dtrain = xgb.DMatrix(x_train, label=np.log(y_train))\n",
    "        self.gbdt = xgb.train(self.param, dtrain, self.nrounds)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.exp(self.gbdt.predict(xgb.DMatrix(x)))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try several models\n",
    "\n",
    "# Put in our parameters. For now these values are the same as those in \n",
    "# https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python\n",
    "\n",
    "# Random Forest parameters\n",
    "et_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators': 100,\n",
    "    'max_features': 0.5,\n",
    "    'max_depth': 12,\n",
    "    'min_samples_leaf': 2,\n",
    "}\n",
    "\n",
    "rf_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators': 100,\n",
    "    'max_features': 0.6,\n",
    "    'max_depth': 12,\n",
    "    'min_samples_leaf': 2,\n",
    "}\n",
    "\n",
    "\n",
    "# Gradient Boosting parameters\n",
    "gb_params = {\n",
    "    'n_estimators': 500,\n",
    "     'max_features': 0.2,\n",
    "    'max_depth': 5,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "mlp_params = {\n",
    "    'hidden_layer_sizes': (4,4)\n",
    "}\n",
    "\n",
    "SEED = 0\n",
    "\n",
    "# Train two versions of each model, one on vote_pct, and one on log_vote_pct\n",
    "# for convenience, also generate predictions for the test data to use later\n",
    "\n",
    "forest = SklearnWrapper(clf=RandomForestRegressor,seed=SEED,params=rf_params)\n",
    "# print(\"Forest Regressor MAE on training data: %f\"% mean_absolute_error(forest.predict(x_holdout),y_holdout))\n",
    "\n",
    "gbm = SklearnWrapper(clf=XGBRegressor,seed=SEED,params=gb_params)\n",
    "# print(\"XGBRegressor MAE on training data: %f\"% mean_absolute_error(gbm.predict(x_holdout),y_holdout))\n",
    "\n",
    "mlp = SklearnWrapper(clf=MLPRegressor,seed=SEED,params=mlp_params)\n",
    "# print(\"MLPRegressor MAE on training data: %f\"% mean_absolute_error(mlp.predict(x_holdout),y_holdout))\n",
    "\n",
    "et = SklearnWrapper(clf=ExtraTreesRegressor,seed=SEED,params=et_params)\n",
    "# print(\"Extra Trees Regressor MAE on training data: %f\"% mean_absolute_error(et.predict(x_holdout),y_holdout))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use XGBoost to blend the base models\n",
    "    \n",
    "lvl1_predictions = pd.DataFrame()\n",
    "\n",
    "lvl1_predictions[\"rf\"] = get_base_predictions(forest)\n",
    "lvl1_predictions[\"xgb\"] = get_base_predictions(gbm)\n",
    "# lvl1_predictions[\"svr\"] = get_base_predictions(svr)\n",
    "lvl1_predictions[\"et\"] = get_base_predictions(et)\n",
    "lvl1_predictions[\"mlp\"] = get_base_predictions(mlp)\n",
    "\n",
    "\n",
    "\n",
    "# look at correlation map of base predictions\n",
    "plt.figure()\n",
    "zz = lvl1_predictions.corr()\n",
    "sns.heatmap(zz,annot=True,cmap=\"RdBu_r\",vmin=-1,vmax=1);\n",
    "\n",
    "# the models are decently correlated with each other. \n",
    "# this is not ideal, but that's life\n",
    "\n",
    "print(lvl1_predictions.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the ensemble \n",
    "xgb_params = {\n",
    "    'seed': 0,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'silent': 1,\n",
    "    'subsample': 0.7,\n",
    "    'learning_rate': 0.075,\n",
    "    'max_depth': 7,\n",
    "    'num_parallel_tree': 1,\n",
    "    'min_child_weight': 1,\n",
    "    'eval_metric': 'mae',\n",
    "    'nrounds': 350\n",
    "}\n",
    "\n",
    "stack = XgbWrapper(seed=SEED,params=xgb_params)\n",
    "\n",
    "stack.train(lvl1_predictions,y_holdout)\n",
    "\n",
    "# feature importances, out of curiosity\n",
    "# print zip(lvl1_predictions.columns,stack.feature_importances)\n",
    "\n",
    "\n",
    "# prepare input predictions for blender\n",
    "x_test_lvl2 = pd.DataFrame(columns = {\n",
    "                                          })\n",
    "\n",
    "x_test_lvl2[\"rf\"] = forest.predict(x_test)\n",
    "x_test_lvl2[\"xgb\"] = gbm.predict(x_test)\n",
    "# x_test_lvl2[\"svr\"] = svr_test_F\n",
    "x_test_lvl2[\"et\"] = et.predict(x_test)\n",
    "x_test_lvl2[\"mlp\"] = mlp.predict(x_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lvl2_prediction = stack.predict(x_test_lvl2)\n",
    "\n",
    "\n",
    "print(\"Averaged MAE %f\" % mean_absolute_error(lvl2_prediction,y_test))\n",
    "\n",
    "test[\"final_prediction\"] = lvl2_prediction\n",
    "\n",
    "\n",
    "# fig = plt.figure()\n",
    "\n",
    "\n",
    "\n",
    "test.plot.scatter(x='pct_of_vote',y='final_prediction')\n",
    "\n",
    "fig = plt.figure(figsize=(16,16))\n",
    "\n",
    "# g = sns.FacetGrid(test,col=\"Year\",height=6,orientation=\"h\")\n",
    "# g.map(sns.barplot, 'player','pct_of_vote')\n",
    "# sns.catplot(x='player', y = 'pct_of_vote',data=test,kind='bar',row='Year',sharex=False,height=14)\n",
    "\n",
    "\n",
    "# its annoyingly difficult to make a facetgrid to show the three years\n",
    "with sns.axes_style(\"darkgrid\"):\n",
    "    for year in [2016,2017,2018]:\n",
    "        t = test[test['Year']==year]\n",
    "        fig = plt.figure()\n",
    "        t[['player','pct_of_vote','final_prediction']].head(10).plot.bar(x='player')\n",
    "        plt.title(\"Year %i\" % year)\n",
    "\n",
    "    \n",
    "\n",
    "# life is hard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some years, such as 2018, are particularly unkind. After a brief investigation, our model seems to strongly overweigh goals, which is not an unreasonable thing to do. The model just sees the statistics in a vacuum, and has no idea about the team's record during that year, or advanced statistics such as WAR that more directly quantify how useful a player was to their team. So there is clearly room for improvement here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make the plotting nicer by only using the last names\n",
    "surnames = [re.search('\\w+ (\\w+)', name).group(1) for name in test[\"player\"]]\n",
    "test[\"player_surname\"] = surnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Showing all three years together\n",
    "f, (ax1, ax2) = plt.subplots(2, 1, figsize=(21, 7), sharex=True)\n",
    "\n",
    "# sns.barplot(x='pct_of_vote', y='final_prediction', data=test[test.loc[:,\"Year\"]==2017], palette=\"vlag\", ax=ax2)\n",
    "sns.barplot(x='player_surname', y='pct_of_vote', data=test, palette=\"deep\", ax=ax1)\n",
    "ax2.axhline(0, color=\"k\", clip_on=False)\n",
    "ax2.set_ylabel(\"% of Vote\")\n",
    "\n",
    "# sns.barplot(x='player', y='final_prediction', data=test[test.loc[:,\"Year\"]==2018], palette=\"deep\", ax=ax3)\n",
    "sns.barplot(x='player_surname', y='final_prediction', data=test, palette=\"deep\", ax=ax2)\n",
    "ax3.axhline(0, color=\"k\", clip_on=False)\n",
    "ax3.set_ylabel(\"Predicted % of Vote\")\n",
    "\n",
    "# Finalize the plot\n",
    "sns.despine(bottom=True)\n",
    "plt.setp(f.axes, yticks=[])\n",
    "plt.tight_layout(h_pad=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
